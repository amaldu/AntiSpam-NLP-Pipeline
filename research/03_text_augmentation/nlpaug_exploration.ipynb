{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used as a base and support in the implementation of the nlpaug library for text augmentation.\n",
    "\n",
    "\n",
    "Inspiration taken from : https://www.kaggle.com/code/andypenrose/text-augmentation-with-nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entry in number a wkly comp to win fa cup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5152</th>\n",
       "      <td>1</td>\n",
       "      <td>this is the numbernd time we have tried number...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5153</th>\n",
       "      <td>0</td>\n",
       "      <td>will you b going to esplanade fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154</th>\n",
       "      <td>0</td>\n",
       "      <td>pity was in mood for that soany other suggestions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5155</th>\n",
       "      <td>0</td>\n",
       "      <td>the guy did some bitching but i acted like id ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5156</th>\n",
       "      <td>0</td>\n",
       "      <td>rofl its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5157 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Category                                            Message\n",
       "0            0  go until jurong point crazy available only in ...\n",
       "1            0                            ok lar joking wif u oni\n",
       "2            1  free entry in number a wkly comp to win fa cup...\n",
       "3            0        u dun say so early hor u c already then say\n",
       "4            0  nah i dont think he goes to usf he lives aroun...\n",
       "...        ...                                                ...\n",
       "5152         1  this is the numbernd time we have tried number...\n",
       "5153         0              will you b going to esplanade fr home\n",
       "5154         0  pity was in mood for that soany other suggestions\n",
       "5155         0  the guy did some bitching but i acted like id ...\n",
       "5156         0                          rofl its true to its name\n",
       "\n",
       "[5157 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../data/silver/df_cleantext_v0.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../utils\"))\n",
    "from experiments_utils import print_and_highlight_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "sample = df['Message'].iloc[:5].astype(str).to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.word.context_word_embs as nawcwe\n",
    "import nlpaug.augmenter.word.word_embs as nawwe\n",
    "import nlpaug.augmenter.word.spelling as naws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeyboardAug method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Original: 20\n",
      "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n",
      "Augmented: 20\n",
      "go until \u001b[31mmufong\u001b[39m point crazy available \u001b[31mLHly\u001b[39m in \u001b[31mbuglC\u001b[39m n \u001b[31mg4eAt\u001b[39m \u001b[31mwLrlF\u001b[39m la e buffet \u001b[31mX8ne\u001b[39m there got amore wat \n",
      "--------------------------------------------------\n",
      "Original: 6\n",
      "ok lar joking wif u oni\n",
      "Augmented: 6\n",
      "ok lar \u001b[31mUouing\u001b[39m wif u oni \n",
      "--------------------------------------------------\n",
      "Original: 28\n",
      "free entry in number a wkly comp to win fa cup final tkts numberst may number text fa to number to receive entry questionstd txt ratetcs apply numberovernumbers\n",
      "Augmented: 28\n",
      "\u001b[31mGre2\u001b[39m entry in \u001b[31mnumh$r\u001b[39m a \u001b[31mwmlg\u001b[39m comp to win fa cup \u001b[31mfJnaP\u001b[39m tkts \u001b[31mn^mfsrst\u001b[39m may \u001b[31mnumged\u001b[39m \u001b[31mteZY\u001b[39m fa to number to receive entry questionstd txt \u001b[31mEatefds\u001b[39m \u001b[31mal9ly\u001b[39m numberovernumbers \n",
      "--------------------------------------------------\n",
      "Original: 11\n",
      "u dun say so early hor u c already then say\n",
      "Augmented: 11\n",
      "u dun say so \u001b[31msxrly\u001b[39m hor u c \u001b[31mX,rFady\u001b[39m \u001b[31mtUeg\u001b[39m say \n",
      "--------------------------------------------------\n",
      "Original: 13\n",
      "nah i dont think he goes to usf he lives around here though\n",
      "Augmented: 13\n",
      "nah i dont \u001b[31mhhJnk\u001b[39m he goes to usf he \u001b[31mlicSs\u001b[39m \u001b[31mAr8und\u001b[39m here \u001b[31mtgouTh\u001b[39m \n"
     ]
    }
   ],
   "source": [
    "aug = nac.KeyboardAug()\n",
    "augmented_texts = aug.augment(sample)\n",
    "\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "1. The original criteria decided to not focus on special chars nor numbers so the cleaning acted accordingly. These two params should be changed. Same for upper/lower chars. \n",
    "2. Many characters of each modified word are changed and the result doesn't look realistic imo. This augmenter technique creates unrealistic results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Original: 20\n",
      "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n",
      "Augmented: 20\n",
      "go until jurong point crazy available only in bugis n great \u001b[31mworir\u001b[39m la e buffet cine there got \u001b[31majorr\u001b[39m wat \n",
      "--------------------------------------------------\n",
      "Original: 6\n",
      "ok lar joking wif u oni\n",
      "Augmented: 6\n",
      "ok lar \u001b[31mioklng\u001b[39m wif u oni \n",
      "--------------------------------------------------\n",
      "Original: 28\n",
      "free entry in number a wkly comp to win fa cup final tkts numberst may number text fa to number to receive entry questionstd txt ratetcs apply numberovernumbers\n",
      "Augmented: 28\n",
      "free \u001b[31msntdy\u001b[39m in number a wkly comp to win fa cup final tkts \u001b[31mnjmbwrat\u001b[39m may \u001b[31mnunbet\u001b[39m text fa to number to receive entry questionstd txt ratetcs apply numberovernumbers \n",
      "--------------------------------------------------\n",
      "Original: 11\n",
      "u dun say so early hor u c already then say\n",
      "Augmented: 11\n",
      "u dun say so \u001b[31meqrpy\u001b[39m hor u c \u001b[31makrsaey\u001b[39m then say \n",
      "--------------------------------------------------\n",
      "Original: 13\n",
      "nah i dont think he goes to usf he lives around here though\n",
      "Augmented: 13\n",
      "nah i dont think he \u001b[31mgkds\u001b[39m to usf he lives around here \u001b[31mthiuvh\u001b[39m \n"
     ]
    }
   ],
   "source": [
    "aug = nac.KeyboardAug(aug_word_p=0.1, include_numeric=False, include_special_char=False, include_upper_case=False)\n",
    "augmented_texts = aug.augment(sample)\n",
    "\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "The misspellings are very hardcore and artificial. They don't make sense to me. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(nac.KeyboardAug)\n",
    "# help(aug.augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpellingAug method\n",
    "\n",
    "This method substitutes word by spelling mistake words dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Original: 20\n",
      "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n",
      "Augmented: 20\n",
      "go \u001b[31mutill\u001b[39m jurong \u001b[31mponint\u001b[39m crazy \u001b[31mavilable\u001b[39m \u001b[31molny\u001b[39m in bugis n great \u001b[31mworlth\u001b[39m la e buffet \u001b[31mcinema\u001b[39m there got amore wat \n",
      "--------------------------------------------------\n",
      "Original: 6\n",
      "ok lar joking wif u oni\n",
      "Augmented: 6\n",
      "ok \u001b[31mlaw\u001b[39m joking wif \u001b[31myou\u001b[39m oni \n",
      "--------------------------------------------------\n",
      "Original: 28\n",
      "free entry in number a wkly comp to win fa cup final tkts numberst may number text fa to number to receive entry questionstd txt ratetcs apply numberovernumbers\n",
      "Augmented: 28\n",
      "free entry in number \u001b[31me\u001b[39m wkly comp \u001b[31mgo\u001b[39m \u001b[31mwinn\u001b[39m fa \u001b[31mcouple\u001b[39m \u001b[31mfinel\u001b[39m tkts numberst \u001b[31mmaybe\u001b[39m \u001b[31mnumbtr\u001b[39m text fa to \u001b[31mnouber\u001b[39m to \u001b[31mreceivement\u001b[39m entry questionstd txt ratetcs apply numberovernumbers \n",
      "--------------------------------------------------\n",
      "Original: 11\n",
      "u dun say so early hor u c already then say\n",
      "Augmented: 11\n",
      "u dun \u001b[31msoy\u001b[39m \u001b[31msoo\u001b[39m early \u001b[31mhot\u001b[39m u c already then \u001b[31mstay\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 13\n",
      "nah i dont think he goes to usf he lives around here though\n",
      "Augmented: 13\n",
      "nah \u001b[31mis\u001b[39m dont think he goes to usf \u001b[31mje\u001b[39m \u001b[31mlife\u001b[39m around \u001b[31mhear\u001b[39m though \n"
     ]
    }
   ],
   "source": [
    "aug = naws.SpellingAug()\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "This method creates more realistic results than the previous technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Original: 20\n",
      "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n",
      "Augmented: 20\n",
      "\u001b[31mago\u001b[39m \u001b[31mtill\u001b[39m jurong point crazy available \u001b[31mnoly\u001b[39m in bugis \u001b[31mNo\u001b[39m \u001b[31mgeart\u001b[39m \u001b[31mwordl\u001b[39m la \u001b[31mold\u001b[39m \u001b[31mbuffe\u001b[39m cine \u001b[31mthear\u001b[39m got amore \u001b[31mwant\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 6\n",
      "ok lar joking wif u oni\n",
      "Augmented: 6\n",
      "ok \u001b[31mlaw\u001b[39m joking \u001b[31mwife\u001b[39m \u001b[31myou\u001b[39m oni \n",
      "--------------------------------------------------\n",
      "Original: 28\n",
      "free entry in number a wkly comp to win fa cup final tkts numberst may number text fa to number to receive entry questionstd txt ratetcs apply numberovernumbers\n",
      "Augmented: 28\n",
      "free entry in \u001b[31munmber\u001b[39m a wkly comp \u001b[31mth\u001b[39m \u001b[31mwinne\u001b[39m fa \u001b[31mcouple\u001b[39m \u001b[31mfinel\u001b[39m tkts numberst \u001b[31mMy\u001b[39m \u001b[31mnummber\u001b[39m text fa \u001b[31mtoa\u001b[39m number \u001b[31mte\u001b[39m receive entry questionstd txt ratetcs \u001b[31mapple\u001b[39m numberovernumbers \n",
      "--------------------------------------------------\n",
      "Original: 11\n",
      "u dun say so early hor u c already then say\n",
      "Augmented: 11\n",
      "u dun \u001b[31msoy\u001b[39m \u001b[31msoI\u001b[39m \u001b[31mearily\u001b[39m \u001b[31mhot\u001b[39m u c already \u001b[31mthin\u001b[39m \u001b[31msaid\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 13\n",
      "nah i dont think he goes to usf he lives around here though\n",
      "Augmented: 13\n",
      "nah i \u001b[31mdon't\u001b[39m \u001b[31mthinck\u001b[39m \u001b[31mHi\u001b[39m \u001b[31mges\u001b[39m \u001b[31mde\u001b[39m usf \u001b[31mh\u001b[39m lives around here \u001b[31mthogh\u001b[39m \n"
     ]
    }
   ],
   "source": [
    "aug = naws.SpellingAug(aug_p = 0.5)\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "1. The results are way more realistic.\n",
    "2. The dictionary includes upper case that should be treated later on.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(naws.SpellingAug())\n",
    "# help(aug.augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SynonymAug\n",
    "\n",
    "Substitute similar word according to WordNet/ PPDB synonym\n",
    "\n",
    "Default is WordNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/maldu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Original: 20\n",
      "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n",
      "Augmented: 21\n",
      "go until jurong point crazy available only in bugis n \u001b[31mnifty\u001b[39m world \u001b[31mpelican\u001b[39m \u001b[31mstate\u001b[39m \u001b[31me\u001b[39m \u001b[31mbuffet\u001b[39m \u001b[31mcine\u001b[39m \u001b[31mthere\u001b[39m \u001b[31mgot\u001b[39m \u001b[31mamore\u001b[39m \u001b[31mwat\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 6\n",
      "ok lar joking wif u oni\n",
      "Augmented: 10\n",
      "\u001b[31mhunky\u001b[39m \u001b[31mdory\u001b[39m \u001b[31mlar\u001b[39m \u001b[31mjoking\u001b[39m \u001b[31mwif\u001b[39m \u001b[31mu\u001b[39m \u001b[31moffice\u001b[39m \u001b[31mof\u001b[39m \u001b[31mnaval\u001b[39m \u001b[31mintelligence\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 28\n",
      "free entry in number a wkly comp to win fa cup final tkts numberst may number text fa to number to receive entry questionstd txt ratetcs apply numberovernumbers\n",
      "Augmented: 30\n",
      "free entry in number a wkly comp to \u001b[31mdeliver\u001b[39m \u001b[31mthe\u001b[39m \u001b[31mgoods\u001b[39m \u001b[31mfa\u001b[39m \u001b[31mcup\u001b[39m \u001b[31mfinal\u001b[39m \u001b[31mtkts\u001b[39m \u001b[31mnumberst\u001b[39m \u001b[31mmay\u001b[39m \u001b[31mnumber\u001b[39m \u001b[31mtext\u001b[39m \u001b[31mfa\u001b[39m to \u001b[31mnumber\u001b[39m \u001b[31mto\u001b[39m \u001b[31mreceive\u001b[39m \u001b[31mentry\u001b[39m \u001b[31mquestionstd\u001b[39m \u001b[31mtxt\u001b[39m \u001b[31mratetcs\u001b[39m \u001b[31mapply\u001b[39m \u001b[31mnumberovernumbers\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 11\n",
      "u dun say so early hor u c already then say\n",
      "Augmented: 13\n",
      "u dun say so \u001b[31mother\u001b[39m hor u \u001b[31mvitamin\u001b[39m \u001b[31mc\u001b[39m \u001b[31malready\u001b[39m \u001b[31mand\u001b[39m \u001b[31mthen\u001b[39m \u001b[31msay\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 13\n",
      "nah i dont think he goes to usf he lives around here though\n",
      "Augmented: 15\n",
      "nah i dont \u001b[31mrecall\u001b[39m he goes to usf \u001b[31matomic\u001b[39m \u001b[31mnumber\u001b[39m \u001b[31m2\u001b[39m \u001b[31mlives\u001b[39m \u001b[31maround\u001b[39m \u001b[31mhither\u001b[39m \u001b[31mthough\u001b[39m \n"
     ]
    }
   ],
   "source": [
    "aug = naw.SynonymAug()\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "1. Some add extra words and I don't understand why\n",
    "\n",
    "- Original: 11\n",
    "- u dun say so early hor u c already then say\n",
    "- Augmented: 12\n",
    "- u dun say so other hor u ampere second already then say \n",
    "\n",
    "2. Translations look more realistic to me \n",
    "3. Checking the params of the function it provides two databases of misspellings 'wordnet' and 'ppdb'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Original: 20\n",
      "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n",
      "Augmented: 20\n",
      "go until jurong point \u001b[31mlooney\u001b[39m available only in bugis n \u001b[31mbig\u001b[39m world \u001b[31mlouisiana\u001b[39m e buffet cine there \u001b[31mpose\u001b[39m amore wat \n",
      "--------------------------------------------------\n",
      "Original: 6\n",
      "ok lar joking wif u oni\n",
      "Augmented: 6\n",
      "ok lar joking wif u oni \n",
      "--------------------------------------------------\n",
      "Original: 28\n",
      "free entry in number a wkly comp to win fa cup final tkts numberst may number text fa to number to receive entry questionstd txt ratetcs apply numberovernumbers\n",
      "Augmented: 28\n",
      "free entry in number a wkly \u001b[31mcomprehensive\u001b[39m to win fa cup final tkts numberst may number text fa to \u001b[31mroutine\u001b[39m to \u001b[31mencounter\u001b[39m entry questionstd txt ratetcs apply numberovernumbers \n",
      "--------------------------------------------------\n",
      "Original: 11\n",
      "u dun say so early hor u c already then say\n",
      "Augmented: 12\n",
      "u dun say so early hor u \u001b[31mdegree\u001b[39m \u001b[31mcentigrade\u001b[39m \u001b[31malready\u001b[39m \u001b[31mthen\u001b[39m \u001b[31mstate\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 13\n",
      "nah i dont think he goes to usf he lives around here though\n",
      "Augmented: 13\n",
      "nah i dont think he goes to usf he lives \u001b[31mabout\u001b[39m \u001b[31mhither\u001b[39m though \n"
     ]
    }
   ],
   "source": [
    "aug = naw.SynonymAug(aug_p= 0.3)\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "1. Indeed we should be careful with the perc of augmented texts because it adds new words changing the length of the sentence. \n",
    "2. Some synonims are not fitting the meaning of the sentence imo.\n",
    "\n",
    "This cannot be used unless we find a way to force 1-to-1 swapping...not even then. We should also restrict the type of synonym to swap to absolute synonyms :\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(naw.SynonymAug())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordEmbeddingAug method\n",
    "\n",
    "This method inserts word randomly by word embeddings similarity. \n",
    "\n",
    ":param str model_type: Model type of word embeddings. Expected values include 'word2vec', 'glove' and 'fasttext'.\n",
    "\n",
    "### Download models\n",
    "\n",
    "The models used are coming from gensim library due to lack of local resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?export=download&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
      "From (redirected): https://drive.google.com/uc?export=download&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&confirm=t&uuid=d99cbbcb-c3b8-4674-b260-d682736f47a2\n",
      "To: /home/maldu/dscience/projects/spam_detector/research/03_text_augmentation/models/GoogleNews-vectors-negative300.bin.gz\n",
      "100%|██████████| 1.65G/1.65G [02:20<00:00, 11.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "# from nlpaug.util.file.download import DownloadUtil\n",
    "\n",
    "# DownloadUtil.download_word2vec(dest_dir='./models')\n",
    "# DownloadUtil.download_glove('glove.6B', './models')\n",
    "# DownloadUtil.download_fasttext('wiki-news-300d-1M', './models')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec\n",
    "\n",
    "Too heavy for my machine, leaving it for Github Actions running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aug = naw.WordEmbsAug(\n",
    "    model_type='word2vec', model_path='./models/GoogleNews-vectors-negative300.bin',\n",
    "    action=\"substitute\")\n",
    "aaugmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.WordEmbsAug(\n",
    "    model_type='word2vec', model_path='./models/GoogleNews-vectors-negative300.bin',\n",
    "    action=\"substitute\", aug_p= 0.3)\n",
    "aaugmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.WordEmbsAug(\n",
    "    model_type='word2vec', model_path='./models/GoogleNews-vectors-negative300.bin',\n",
    "    action=\"substitute\", aug_p= 0.8)\n",
    "aaugmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glove 6B 100d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aug = naw.WordEmbsAug(\n",
    "    model_type='glove', model_path='./models/glove.6B.100d.txt',\n",
    "    action=\"substitute\")\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aug = naw.WordEmbsAug(\n",
    "    model_type='glove', model_path='./models/glove.6B.100d.txt',\n",
    "    action=\"substitute\",  aug_p= 0.3)\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aug = naw.WordEmbsAug(\n",
    "    model_type='glove', model_path='./models/glove.6B.100d.txt',\n",
    "    action=\"substitute\",  aug_p= 0.8)\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.WordEmbsAug(\n",
    "    model_type='fasttext', model_path='./models/wiki-new-300d-1M.vec',\n",
    "    action=\"substitute\")\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aug = naw.WordEmbsAug(\n",
    "    model_type='fasttext', model_path='./models/wiki-new-300d-1M.vec',\n",
    "    action=\"substitute\", aug_p= 0.3)\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.WordEmbsAug(\n",
    "    model_type='word2vec', model='nlpaug.model.word_embs.nmw.Word2vec()',\n",
    "    action=\"substitute\", aug_p= 0.8)\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get_vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnlpaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmenter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnaw\u001b[39;00m\n\u001b[1;32m      2\u001b[0m aug \u001b[38;5;241m=\u001b[39m naw\u001b[38;5;241m.\u001b[39mWordEmbsAug(model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword2vec\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnlpaug.model.word_embs.nmw.Word2vec()\u001b[39m\u001b[38;5;124m'\u001b[39m,action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubstitute\u001b[39m\u001b[38;5;124m\"\u001b[39m, aug_p\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m augmented_texts \u001b[38;5;241m=\u001b[39m \u001b[43maug\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m print_and_highlight_diff(sample, augmented_texts)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/nlpaug/base_augmenter.py:119\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[0;34m(self, data, n, num_thread)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Single input with/without multiple input\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Single Thread\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_thread \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m         augmented_results \u001b[38;5;241m=\u001b[39m [action_fx(clean_data) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# Multi Thread\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         augmented_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parallel_augment(action_fx, clean_data, n\u001b[38;5;241m=\u001b[39mn, num_thread\u001b[38;5;241m=\u001b[39mnum_thread)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/nlpaug/base_augmenter.py:119\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Single input with/without multiple input\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Single Thread\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_thread \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m         augmented_results \u001b[38;5;241m=\u001b[39m [\u001b[43maction_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# Multi Thread\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         augmented_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parallel_augment(action_fx, clean_data, n\u001b[38;5;241m=\u001b[39mn, num_thread\u001b[38;5;241m=\u001b[39mnum_thread)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/nlpaug/augmenter/word/word_embs.py:147\u001b[0m, in \u001b[0;36mWordEmbsAug.substitute\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    144\u001b[0m change_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    145\u001b[0m doc \u001b[38;5;241m=\u001b[39m Doc(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(data))\n\u001b[0;32m--> 147\u001b[0m aug_idxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_aug_idxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_original_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aug_idxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(aug_idxes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_detail:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/nlpaug/augmenter/word/word_augmenter.py:77\u001b[0m, in \u001b[0;36mWordAugmenter._get_aug_idxes\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     75\u001b[0m aug_cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_aug_cnt(\u001b[38;5;28mlen\u001b[39m(tokens))\n\u001b[1;32m     76\u001b[0m word_idxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_skip_aug(tokens)\n\u001b[0;32m---> 77\u001b[0m word_idxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_aug\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_idxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word_idxes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/nlpaug/augmenter/word/word_embs.py:107\u001b[0m, in \u001b[0;36mWordEmbsAug.skip_aug\u001b[0;34m(self, token_idxes, tokens)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token_idx \u001b[38;5;129;01min\u001b[39;00m token_idxes:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Some words do not come with vector. It will be excluded in lucky draw.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     word \u001b[38;5;241m=\u001b[39m tokens[token_idx]\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab\u001b[49m():\n\u001b[1;32m    108\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(token_idx)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get_vocab'"
     ]
    }
   ],
   "source": [
    "# import nlpaug.augmenter.word as naw\n",
    "# aug = naw.WordEmbsAug(model_type='word2vec', model='nlpaug.model.word_embs.nmw.Word2vec()',action=\"substitute\", aug_p= 0.8)\n",
    "# augmented_texts = aug.augment(sample)\n",
    "# print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(naw.WordEmbsAug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.WordEmbsAug(\n",
    "    model_type='fasttext', model_path='./models/wiki-new-300d-1M.vec',\n",
    "    action=\"substitute\", aug_p= 0.8)\n",
    "aaugmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ContextualWordEmbs method\n",
    "\n",
    "I'll try SqueezeBERT since the resources are limited "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='distilbert-base-uncased', action=\"substitute\", aug_p= 0.3)\n",
    "augmented_text = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Original: 1\n",
      "Y\n",
      "Augmented: 5\n",
      "\u001b[31mmy\u001b[39m \u001b[31msample\u001b[39m \u001b[31mjust\u001b[39m \u001b[31mgoes\u001b[39m \u001b[31mhere\u001b[39m \n"
     ]
    }
   ],
   "source": [
    "aug = nawcwe.ContextualWordEmbsAug(\n",
    "    model_path=model_dir, \n",
    "    action ='substitute', \n",
    "    aug_p= 0.8)\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(nawcwe.ContextualWordEmbsAug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam-detector-P2ybB3t6-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
