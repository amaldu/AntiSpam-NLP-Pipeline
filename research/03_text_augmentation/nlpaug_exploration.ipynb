{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used as a base and support in the implementation of the nlpaug library for text augmentation.\n",
    "\n",
    "\n",
    "Inspiration taken from : https://www.kaggle.com/code/andypenrose/text-augmentation-with-nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entry in number a wkly comp to win fa cup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5152</th>\n",
       "      <td>1</td>\n",
       "      <td>this is the numbernd time we have tried number...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5153</th>\n",
       "      <td>0</td>\n",
       "      <td>will you b going to esplanade fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154</th>\n",
       "      <td>0</td>\n",
       "      <td>pity was in mood for that soany other suggestions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5155</th>\n",
       "      <td>0</td>\n",
       "      <td>the guy did some bitching but i acted like id ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5156</th>\n",
       "      <td>0</td>\n",
       "      <td>rofl its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5157 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Category                                            Message\n",
       "0            0  go until jurong point crazy available only in ...\n",
       "1            0                            ok lar joking wif u oni\n",
       "2            1  free entry in number a wkly comp to win fa cup...\n",
       "3            0        u dun say so early hor u c already then say\n",
       "4            0  nah i dont think he goes to usf he lives aroun...\n",
       "...        ...                                                ...\n",
       "5152         1  this is the numbernd time we have tried number...\n",
       "5153         0              will you b going to esplanade fr home\n",
       "5154         0  pity was in mood for that soany other suggestions\n",
       "5155         0  the guy did some bitching but i acted like id ...\n",
       "5156         0                          rofl its true to its name\n",
       "\n",
       "[5157 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../data/silver/df_cleantext_v0.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../utils\"))\n",
    "from experiments_utils import print_and_highlight_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "sample = df['Message'].iloc[:5].astype(str).to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.word.context_word_embs as nawcwe\n",
    "import nlpaug.augmenter.word.word_embs as nawwe\n",
    "import nlpaug.augmenter.word.spelling as naws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeyboardAug method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = nac.KeyboardAug()\n",
    "augmented_texts = aug.augment(sample)\n",
    "\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "1. The original criteria decided to not focus on special chars nor numbers so the cleaning acted accordingly. These two params should be changed. Same for upper/lower chars. \n",
    "2. Many characters of each modified word are changed and the result doesn't look realistic imo. This augmenter technique creates unrealistic results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = nac.KeyboardAug(aug_word_p=0.1, include_numeric=False, include_special_char=False, include_upper_case=False)\n",
    "augmented_texts = aug.augment(sample)\n",
    "\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "The misspellings are very hardcore and artificial. They don't make sense to me. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(nac.KeyboardAug)\n",
    "# help(aug.augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpellingAug method\n",
    "\n",
    "This method substitutes word by spelling mistake words dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naws.SpellingAug()\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "This method creates more realistic results than the previous technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naws.SpellingAug(aug_p = 0.5)\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "1. The results are way more realistic.\n",
    "2. The dictionary includes upper case that should be treated later on.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(naws.SpellingAug())\n",
    "# help(aug.augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SynonymAug\n",
    "\n",
    "Substitute similar word according to WordNet/ PPDB synonym\n",
    "\n",
    "Default is WordNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.SynonymAug()\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "1. Some add extra words and I don't understand why\n",
    "\n",
    "- Original: 11\n",
    "- u dun say so early hor u c already then say\n",
    "- Augmented: 12\n",
    "- u dun say so other hor u ampere second already then say \n",
    "\n",
    "2. Translations look more realistic to me \n",
    "3. Checking the params of the function it provides two databases of misspellings 'wordnet' and 'ppdb'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.SynonymAug(aug_p= 0.3)\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "1. Indeed we should be careful with the perc of augmented texts because it adds new words changing the length of the sentence. \n",
    "2. Some synonims are not fitting the meaning of the sentence imo.\n",
    "\n",
    "This cannot be used unless we find a way to force 1-to-1 swapping...not even then. We should also restrict the type of synonym to swap to absolute synonyms :\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(naw.SynonymAug())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordEmbeddingAug method\n",
    "\n",
    "This method inserts word randomly by word embeddings similarity. \n",
    "\n",
    ":param str model_type: Model type of word embeddings. Expected values include 'word2vec', 'glove' and 'fasttext'.\n",
    "\n",
    "### Download models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "models_dir = \"./models\"\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# model_path = hf_hub_download(\n",
    "#     repo_id=\"NathaNn1111/word2vec-google-news-negative-300-bin\", \n",
    "#     filename=\"GoogleNews-vectors-negative300.bin\",\n",
    "#     local_dir=models_dir\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "glove_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "zip_path = \"./models/glove.6B.zip\"\n",
    "response = requests.get(glove_url)\n",
    "with open(zip_path, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./models\")\n",
    "os.remove(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388bb3483fab4e7ab0d1b46fd6ae4934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.bin:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo descargado y guardado en: ./models/models--facebook--fasttext-language-identification/snapshots/3af127d4124fc58b75666f3594bb5143b9757e78/model.bin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"facebook/fasttext-language-identification\",\n",
    "    filename=\"model.bin\",\n",
    "    cache_dir=models_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec\n",
    "\n",
    "Too heavy for my machine, leaving it for Github Actions running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.WordEmbsAug(\n",
    "    model_type='word2vec', \n",
    "    model_path=\"./models/GoogleNews-vectors-negative300.bin\",\n",
    "    action=\"substitute\")\n",
    "augmented_text = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glove 6B 100d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.WordEmbsAug(\n",
    "    model_type='glove', \n",
    "    model_path=\"./models/glove.6B.1000d.txt\",\n",
    "    action=\"substitute\")\n",
    "augmented_text = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'naw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m aug \u001b[38;5;241m=\u001b[39m \u001b[43mnaw\u001b[49m\u001b[38;5;241m.\u001b[39mWordEmbsAug(\n\u001b[1;32m      2\u001b[0m     model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfasttext\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m     model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/models--facebook--fasttext-language-identification/.no_exist/fasttext.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \n\u001b[1;32m      4\u001b[0m     action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubstitute\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m augmented_text \u001b[38;5;241m=\u001b[39m aug\u001b[38;5;241m.\u001b[39maugment(sample)\n\u001b[1;32m      8\u001b[0m print_and_highlight_diff(sample, augmented_texts)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'naw' is not defined"
     ]
    }
   ],
   "source": [
    "aug = naw.WordEmbsAug(\n",
    "    model_type='fasttext', \n",
    "    model_path=\"./models/models--facebook--fasttext-language-identification/.no_exist/3af127d4124fc58b75666f3594bb5143b9757e78/fasttext.bin\",  \n",
    "    action=\"substitute\"  \n",
    ")\n",
    "\n",
    "augmented_text = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(naw.WordEmbsAug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ContextualWordEmbs method\n",
    "\n",
    "I'll try SqueezeBERT since I'm trying to run as many things in local as I can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_dir = \"./models/squeezebert\"\n",
    "\n",
    "model_name = \"squeezebert/squeezebert-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "model.save_pretrained(model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = nawcwe.ContextualWordEmbsAug(model_path=model_dir, action ='substitute', aug_p= 0.3)\n",
    "augmented_texts = aug.augment(sample)\n",
    "# augmented_texts = [x.replace(\" ' \", \"'\") for x in augmented_texts]\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nawcwe.ContextualWordEmbsAug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam-detector-P2ybB3t6-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
