{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used as a base and support in the implementation of the nlpaug library for text augmentation.\n",
    "\n",
    "\n",
    "Inspiration taken from : https://www.kaggle.com/code/andypenrose/text-augmentation-with-nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entry in number a wkly comp to win fa cup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5152</th>\n",
       "      <td>1</td>\n",
       "      <td>this is the numbernd time we have tried number...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5153</th>\n",
       "      <td>0</td>\n",
       "      <td>will you b going to esplanade fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154</th>\n",
       "      <td>0</td>\n",
       "      <td>pity was in mood for that soany other suggestions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5155</th>\n",
       "      <td>0</td>\n",
       "      <td>the guy did some bitching but i acted like id ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5156</th>\n",
       "      <td>0</td>\n",
       "      <td>rofl its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5157 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Category                                            Message\n",
       "0            0  go until jurong point crazy available only in ...\n",
       "1            0                            ok lar joking wif u oni\n",
       "2            1  free entry in number a wkly comp to win fa cup...\n",
       "3            0        u dun say so early hor u c already then say\n",
       "4            0  nah i dont think he goes to usf he lives aroun...\n",
       "...        ...                                                ...\n",
       "5152         1  this is the numbernd time we have tried number...\n",
       "5153         0              will you b going to esplanade fr home\n",
       "5154         0  pity was in mood for that soany other suggestions\n",
       "5155         0  the guy did some bitching but i acted like id ...\n",
       "5156         0                          rofl its true to its name\n",
       "\n",
       "[5157 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../data/silver/df_cleantext_v0.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../utils\"))\n",
    "from experiments_utils import print_and_highlight_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "sample = df['Message'].iloc[:5].astype(str).to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.word.context_word_embs as nawcwe\n",
    "import nlpaug.augmenter.word.word_embs as nawwe\n",
    "import nlpaug.augmenter.word.spelling as naws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeyboardAug method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Original: 20\n",
      "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n",
      "Augmented: 20\n",
      "go \u001b[31muGtiK\u001b[39m jurong point \u001b[31mcraAt\u001b[39m \u001b[31mavai<QblF\u001b[39m only in \u001b[31mbutiD\u001b[39m n \u001b[31mfreaf\u001b[39m world la e \u001b[31mbIff3t\u001b[39m cine there got amore wat \n",
      "--------------------------------------------------\n",
      "Original: 6\n",
      "ok lar joking wif u oni\n",
      "Augmented: 6\n",
      "ok lar \u001b[31muoOing\u001b[39m wif u oni \n",
      "--------------------------------------------------\n",
      "Original: 28\n",
      "free entry in number a wkly comp to win fa cup final tkts numberst may number text fa to number to receive entry questionstd txt ratetcs apply numberovernumbers\n",
      "Augmented: 28\n",
      "\u001b[31mCGee\u001b[39m \u001b[31memtrt\u001b[39m in number a \u001b[31mwllH\u001b[39m \u001b[31mcoNl\u001b[39m to win fa cup \u001b[31mBinQl\u001b[39m tkts numberst may \u001b[31mn&mbFr\u001b[39m \u001b[31mtDxF\u001b[39m fa to number to receive entry \u001b[31mquSstioGcGd\u001b[39m txt ratetcs apply \u001b[31mM&mbeDoverjumbefw\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 11\n",
      "u dun say so early hor u c already then say\n",
      "Augmented: 11\n",
      "u dun say so \u001b[31mWarlg\u001b[39m hor u c \u001b[31mXlrewdJ\u001b[39m \u001b[31m$h2n\u001b[39m say \n",
      "--------------------------------------------------\n",
      "Original: 13\n",
      "nah i dont think he goes to usf he lives around here though\n",
      "Augmented: 13\n",
      "nah i \u001b[31mdoh^\u001b[39m \u001b[31mth8n,\u001b[39m he goes to usf he lives around \u001b[31mhfr@\u001b[39m \u001b[31mth0urh\u001b[39m \n"
     ]
    }
   ],
   "source": [
    "aug = nac.KeyboardAug()\n",
    "augmented_texts = aug.augment(sample)\n",
    "\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "1. The original criteria decided to not focus on special chars nor numbers so the cleaning acted accordingly. These two params should be changed. Same for upper/lower chars. \n",
    "2. Many characters of each modified word are changed and the result doesn't look realistic imo. This augmenter technique creates unrealistic results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Original: 20\n",
      "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n",
      "Augmented: 20\n",
      "go until \u001b[31mnugong\u001b[39m point crazy available only in bugis n great world la e \u001b[31mgiffet\u001b[39m cine there got amore wat \n",
      "--------------------------------------------------\n",
      "Original: 6\n",
      "ok lar joking wif u oni\n",
      "Augmented: 6\n",
      "ok lar \u001b[31mjoukng\u001b[39m wif u oni \n",
      "--------------------------------------------------\n",
      "Original: 28\n",
      "free entry in number a wkly comp to win fa cup final tkts numberst may number text fa to number to receive entry questionstd txt ratetcs apply numberovernumbers\n",
      "Augmented: 28\n",
      "\u001b[31mffde\u001b[39m entry in number a wkly comp to win fa cup final tkts numberst may number text fa to \u001b[31mnumvsr\u001b[39m to receive entry questionstd txt ratetcs apply \u001b[31mnumnwrkvernumndfs\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 11\n",
      "u dun say so early hor u c already then say\n",
      "Augmented: 11\n",
      "u dun say so \u001b[31mfagly\u001b[39m hor u c already \u001b[31mtusn\u001b[39m say \n",
      "--------------------------------------------------\n",
      "Original: 13\n",
      "nah i dont think he goes to usf he lives around here though\n",
      "Augmented: 13\n",
      "nah i \u001b[31mdknf\u001b[39m think he \u001b[31mgofx\u001b[39m to usf he lives around here though \n"
     ]
    }
   ],
   "source": [
    "aug = nac.KeyboardAug(aug_word_p=0.1, include_numeric=False, include_special_char=False, include_upper_case=False)\n",
    "augmented_texts = aug.augment(sample)\n",
    "\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "The misspellings are very hardcore and artificial. They don't make sense to me. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(nac.KeyboardAug)\n",
    "# help(aug.augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpellingAug method\n",
    "\n",
    "This method substitutes word by spelling mistake words dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Original: 20\n",
      "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n",
      "Augmented: 20\n",
      "go until jurong point \u001b[31mcraezy\u001b[39m available only in bugis n \u001b[31mgrait\u001b[39m world la \u001b[31mand\u001b[39m buffet \u001b[31mcinema\u001b[39m \u001b[31mtrere\u001b[39m got amore \u001b[31mwant\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 6\n",
      "ok lar joking wif u oni\n",
      "Augmented: 6\n",
      "\u001b[31mOK)][[...\u001b[39m lar joking \u001b[31mwife\u001b[39m u oni \n",
      "--------------------------------------------------\n",
      "Original: 28\n",
      "free entry in number a wkly comp to win fa cup final tkts numberst may number text fa to number to receive entry questionstd txt ratetcs apply numberovernumbers\n",
      "Augmented: 28\n",
      "\u001b[31mfreer\u001b[39m entry \u001b[31mil\u001b[39m \u001b[31mlnumber\u001b[39m \u001b[31mal\u001b[39m wkly comp \u001b[31mtoo.\u001b[39m \u001b[31mwind\u001b[39m fa \u001b[31mcoop\u001b[39m final tkts numberst may \u001b[31mnumenber\u001b[39m text fa \u001b[31mtj\u001b[39m number to receive entry questionstd txt ratetcs apply numberovernumbers \n",
      "--------------------------------------------------\n",
      "Original: 11\n",
      "u dun say so early hor u c already then say\n",
      "Augmented: 11\n",
      "u dun \u001b[31msoy\u001b[39m \u001b[31msaw\u001b[39m \u001b[31morly\u001b[39m hor u c already then \u001b[31msoy\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 13\n",
      "nah i dont think he goes to usf he lives around here though\n",
      "Augmented: 13\n",
      "nah i \u001b[31mdoesn't\u001b[39m think he goes to usf \u001b[31mbe\u001b[39m \u001b[31mlive/lifes\u001b[39m \u001b[31mauound\u001b[39m here though \n"
     ]
    }
   ],
   "source": [
    "aug = naws.SpellingAug()\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "This method creates more realistic results than the previous technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Original: 20\n",
      "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n",
      "Augmented: 20\n",
      "\u001b[31mgou\u001b[39m until jurong point \u001b[31mcarzy\u001b[39m \u001b[31mavalible\u001b[39m \u001b[31mony\u001b[39m in bugis \u001b[31min\u001b[39m \u001b[31mgrate\u001b[39m world la \u001b[31mHe\u001b[39m buffet \u001b[31mcinema\u001b[39m there \u001b[31mgotten\u001b[39m amore \u001b[31meat\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 6\n",
      "ok lar joking wif u oni\n",
      "Augmented: 6\n",
      "\u001b[31mOk\u001b[39m \u001b[31mlaw\u001b[39m joking wif \u001b[31mYou\u001b[39m oni \n",
      "--------------------------------------------------\n",
      "Original: 28\n",
      "free entry in number a wkly comp to win fa cup final tkts numberst may number text fa to number to receive entry questionstd txt ratetcs apply numberovernumbers\n",
      "Augmented: 28\n",
      "free entry \u001b[31mim\u001b[39m \u001b[31mnubmer\u001b[39m a wkly comp \u001b[31mot\u001b[39m \u001b[31mwinne\u001b[39m fa cup final tkts numberst \u001b[31mmaybe\u001b[39m \u001b[31mnamber's\u001b[39m text fa \u001b[31mte\u001b[39m \u001b[31mnummer\u001b[39m \u001b[31mro\u001b[39m receive entry questionstd txt ratetcs \u001b[31mapplying\u001b[39m numberovernumbers \n",
      "--------------------------------------------------\n",
      "Original: 11\n",
      "u dun say so early hor u c already then say\n",
      "Augmented: 11\n",
      "\u001b[31mYou\u001b[39m dun \u001b[31msoy\u001b[39m \u001b[31ms\u001b[39m early hor \u001b[31myou\u001b[39m c already \u001b[31mthent\u001b[39m \u001b[31mstay\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 13\n",
      "nah i dont think he goes to usf he lives around here though\n",
      "Augmented: 13\n",
      "nah \u001b[31mIâ€™ve\u001b[39m \u001b[31mwon't\u001b[39m \u001b[31mthonk\u001b[39m \u001b[31mihe\u001b[39m \u001b[31mGes\u001b[39m to usf he lives \u001b[31marond\u001b[39m \u001b[31mhear\u001b[39m though \n"
     ]
    }
   ],
   "source": [
    "aug = naws.SpellingAug(aug_p = 0.5)\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "1. The results are way more realistic.\n",
    "2. The dictionary includes upper case that should be treated later on.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(naws.SpellingAug())\n",
    "# help(aug.augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SynonymAug\n",
    "\n",
    "Substitute similar word according to WordNet/ PPDB synonym\n",
    "\n",
    "Default is WordNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/maldu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Original: 20\n",
      "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n",
      "Augmented: 21\n",
      "go \u001b[31mbad\u001b[39m \u001b[31muntil\u001b[39m \u001b[31mjurong\u001b[39m \u001b[31mperiod\u001b[39m \u001b[31mweirdo\u001b[39m \u001b[31mavailable\u001b[39m \u001b[31monly\u001b[39m \u001b[31min\u001b[39m \u001b[31mbugis\u001b[39m \u001b[31mn\u001b[39m \u001b[31mgreat\u001b[39m \u001b[31mworld\u001b[39m \u001b[31mla\u001b[39m \u001b[31me\u001b[39m \u001b[31mbuffet\u001b[39m \u001b[31mcine\u001b[39m \u001b[31mthere\u001b[39m \u001b[31mget\u001b[39m \u001b[31mamore\u001b[39m \u001b[31mwat\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 6\n",
      "ok lar joking wif u oni\n",
      "Augmented: 6\n",
      "ok lar joking wif u oni \n",
      "--------------------------------------------------\n",
      "Original: 28\n",
      "free entry in number a wkly comp to win fa cup final tkts numberst may number text fa to number to receive entry questionstd txt ratetcs apply numberovernumbers\n",
      "Augmented: 29\n",
      "free \u001b[31mentrance\u001b[39m in number a wkly \u001b[31mcomprehensive\u001b[39m to win fa cup \u001b[31mlast\u001b[39m tkts numberst may \u001b[31mtelephone\u001b[39m \u001b[31mnumber\u001b[39m \u001b[31mtext\u001b[39m \u001b[31mfa\u001b[39m \u001b[31mto\u001b[39m \u001b[31mnumber\u001b[39m \u001b[31mto\u001b[39m \u001b[31mget\u001b[39m \u001b[31mentry\u001b[39m \u001b[31mquestionstd\u001b[39m \u001b[31mtxt\u001b[39m \u001b[31mratetcs\u001b[39m \u001b[31mapply\u001b[39m \u001b[31mnumberovernumbers\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 11\n",
      "u dun say so early hor u c already then say\n",
      "Augmented: 12\n",
      "u dun say \u001b[31mthus\u001b[39m early hor u c already \u001b[31mand\u001b[39m \u001b[31mthen\u001b[39m \u001b[31msay\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 13\n",
      "nah i dont think he goes to usf he lives around here though\n",
      "Augmented: 13\n",
      "nah \u001b[31mace\u001b[39m dont think he \u001b[31mlive\u001b[39m to usf he lives around here though \n"
     ]
    }
   ],
   "source": [
    "aug = naw.SynonymAug()\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "1. Some add extra words and I don't understand why\n",
    "\n",
    "- Original: 11\n",
    "- u dun say so early hor u c already then say\n",
    "- Augmented: 12\n",
    "- u dun say so other hor u ampere second already then say \n",
    "\n",
    "2. Translations look more realistic to me \n",
    "3. Checking the params of the function it provides two databases of misspellings 'wordnet' and 'ppdb'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Original: 20\n",
      "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n",
      "Augmented: 21\n",
      "go until jurong \u001b[31mfull\u001b[39m \u001b[31mstop\u001b[39m \u001b[31mcrazy\u001b[39m \u001b[31mavailable\u001b[39m \u001b[31mentirely\u001b[39m \u001b[31min\u001b[39m \u001b[31mbugis\u001b[39m \u001b[31mn\u001b[39m \u001b[31mgreat\u001b[39m \u001b[31mreality\u001b[39m \u001b[31mlanthanum\u001b[39m \u001b[31me\u001b[39m \u001b[31mbuffet\u001b[39m \u001b[31mcine\u001b[39m \u001b[31mthere\u001b[39m \u001b[31mgot\u001b[39m \u001b[31mamore\u001b[39m \u001b[31mwat\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 6\n",
      "ok lar joking wif u oni\n",
      "Augmented: 7\n",
      "\u001b[31mall\u001b[39m \u001b[31mright\u001b[39m \u001b[31mlar\u001b[39m \u001b[31mjoking\u001b[39m \u001b[31mwif\u001b[39m \u001b[31mu\u001b[39m \u001b[31moni\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 28\n",
      "free entry in number a wkly comp to win fa cup final tkts numberst may number text fa to number to receive entry questionstd txt ratetcs apply numberovernumbers\n",
      "Augmented: 29\n",
      "\u001b[31mgratuitous\u001b[39m entry in number a wkly comp to win fa cup final tkts numberst \u001b[31menglish\u001b[39m \u001b[31mhawthorn\u001b[39m \u001b[31mnumber\u001b[39m \u001b[31mtext\u001b[39m \u001b[31mfa\u001b[39m \u001b[31mto\u001b[39m \u001b[31mact\u001b[39m \u001b[31mto\u001b[39m \u001b[31mreceive\u001b[39m \u001b[31mentry\u001b[39m \u001b[31mquestionstd\u001b[39m \u001b[31mtxt\u001b[39m \u001b[31mratetcs\u001b[39m \u001b[31mapply\u001b[39m \u001b[31mnumberovernumbers\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 11\n",
      "u dun say so early hor u c already then say\n",
      "Augmented: 12\n",
      "u dun say \u001b[31mindeed\u001b[39m early hor u c already \u001b[31mand\u001b[39m \u001b[31mso\u001b[39m \u001b[31menjoin\u001b[39m \n",
      "--------------------------------------------------\n",
      "Original: 13\n",
      "nah i dont think he goes to usf he lives around here though\n",
      "Augmented: 18\n",
      "nah \u001b[31matomic\u001b[39m \u001b[31mnumber\u001b[39m \u001b[31m53\u001b[39m \u001b[31mdont\u001b[39m \u001b[31mthink\u001b[39m \u001b[31matomic\u001b[39m \u001b[31mnumber\u001b[39m \u001b[31m2\u001b[39m \u001b[31mgoes\u001b[39m \u001b[31mto\u001b[39m \u001b[31musf\u001b[39m \u001b[31mhe\u001b[39m \u001b[31mlives\u001b[39m \u001b[31mclose\u001b[39m \u001b[31mto\u001b[39m \u001b[31mhere\u001b[39m \u001b[31mthough\u001b[39m \n"
     ]
    }
   ],
   "source": [
    "aug = naw.SynonymAug(aug_p= 0.3)\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "1. Indeed we should be careful with the perc of augmented texts because it adds new words changing the length of the sentence. \n",
    "2. Some synonims are not fitting the meaning of the sentence imo.\n",
    "\n",
    "This cannot be used unless we find a way to force 1-to-1 swapping...not even then. We should also restrict the type of synonym to swap to absolute synonyms :\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(naw.SynonymAug())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordEmbeddingAug method\n",
    "\n",
    "This method inserts word randomly by word embeddings similarity. \n",
    "\n",
    ":param str model_type: Model type of word embeddings. Expected values include 'word2vec', 'glove' and 'fasttext'.\n",
    "\n",
    "### Download models\n",
    "\n",
    "The models used are coming from gensim library due to lack of local resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpaug.util.file.download import DownloadUtil\n",
    "\n",
    "DownloadUtil.download_word2vec(dest_dir='./models')\n",
    "DownloadUtil.download_glove('glove.6B', './models')\n",
    "DownloadUtil.download_fasttext('wiki-news-300d-1M', './models')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec\n",
    "\n",
    "Too heavy for my machine, leaving it for Github Actions running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03 17:02:05,707 - INFO - loading projection weights from ./models/GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './models/GoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m aug \u001b[38;5;241m=\u001b[39m \u001b[43mnaw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWordEmbsAug\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mword2vec\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./models/GoogleNews-vectors-negative300.bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubstitute\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# augmented_texts = aug.augment(sample)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print_and_highlight_diff(sample, augmented_texts)\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/nlpaug/augmenter/word/word_embs.py:88\u001b[0m, in \u001b[0;36mWordEmbsAug.__init__\u001b[0;34m(self, model_type, model_path, model, action, name, aug_min, aug_max, aug_p, top_k, n_gram_separator, stopwords, tokenizer, reverse_tokenizer, force_reload, stopwords_regex, verbose, skip_check)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_validate()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_reload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_check\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_check\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/nlpaug/augmenter/word/word_embs.py:99\u001b[0m, in \u001b[0;36mWordEmbsAug.get_model\u001b[0;34m(cls, model_path, model_type, force_reload, top_k, skip_check)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;28mcls\u001b[39m, model_path, model_type, force_reload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, skip_check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minit_word_embs_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_check\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_check\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/nlpaug/augmenter/word/word_embs.py:23\u001b[0m, in \u001b[0;36minit_word_embs_model\u001b[0;34m(model_path, model_type, force_reload, top_k, skip_check)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword2vec\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     22\u001b[0m     model \u001b[38;5;241m=\u001b[39m nmw\u001b[38;5;241m.\u001b[39mWord2vec(top_k\u001b[38;5;241m=\u001b[39mtop_k, skip_check\u001b[38;5;241m=\u001b[39mskip_check)\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglove\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     25\u001b[0m     model \u001b[38;5;241m=\u001b[39m nmw\u001b[38;5;241m.\u001b[39mGloVe(top_k\u001b[38;5;241m=\u001b[39mtop_k, skip_check\u001b[38;5;241m=\u001b[39mskip_check)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/nlpaug/model/word_embs/word2vec.py:24\u001b[0m, in \u001b[0;36mWord2vec.read\u001b[0;34m(self, file_path, max_num_vector)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path, max_num_vector\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_num_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_read()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/gensim/models/keyedvectors.py:2048\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2045\u001b[0m             counts[word] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(count)\n\u001b[1;32m   2047\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading projection weights from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, fname)\n\u001b[0;32m-> 2048\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_header:\n\u001b[1;32m   2050\u001b[0m         \u001b[38;5;66;03m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/smart_open/smart_open_lib.py:177\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 177\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/smart_open/smart_open_lib.py:363\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    361\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_builtin_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './models/GoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "\n",
    "aug = naw.WordEmbsAug(\n",
    "    model_type='word2vec', model_path='./models/GoogleNews-vectors-negative300.bin',\n",
    "    action=\"substitute\")\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.WordEmbsAug(\n",
    "    model_type='word2vec', model_path='./models/GoogleNews-vectors-negative300.bin',\n",
    "    action=\"substitute\", aug_p= 0.3)\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.WordEmbsAug(\n",
    "    model_type='word2vec', model_path='./models/GoogleNews-vectors-negative300.bin',\n",
    "    action=\"substitute\", aug_p= 0.8)\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glove 6B 100d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aug = naw.WordEmbsAug(\n",
    "    model_type='glove', model_path='./models/glove.6B.100d.txt',\n",
    "    action=\"substitute\")\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aug = naw.WordEmbsAug(\n",
    "    model_type='glove', model_path='./models/glove.6B.100d.txt',\n",
    "    action=\"substitute\",  aug_p= 0.3)\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aug = naw.WordEmbsAug(\n",
    "    model_type='glove', model_path='./models/glove.6B.100d.txt',\n",
    "    action=\"substitute\",  aug_p= 0.8)\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.WordEmbsAug(\n",
    "    model_type='fasttext', model_path='./models/wiki-new-300d-1M.vec',\n",
    "    action=\"substitute\")\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aug = naw.WordEmbsAug(\n",
    "    model_type='fasttext', model_path='./models/wiki-new-300d-1M.vec',\n",
    "    action=\"substitute\", aug_p= 0.3)\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.WordEmbsAug(\n",
    "    model_type='word2vec', model='nlpaug.model.word_embs.nmw.Word2vec()',\n",
    "    action=\"substitute\", aug_p= 0.8)\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get_vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnlpaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmenter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnaw\u001b[39;00m\n\u001b[1;32m      2\u001b[0m aug \u001b[38;5;241m=\u001b[39m naw\u001b[38;5;241m.\u001b[39mWordEmbsAug(model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword2vec\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnlpaug.model.word_embs.nmw.Word2vec()\u001b[39m\u001b[38;5;124m'\u001b[39m,action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubstitute\u001b[39m\u001b[38;5;124m\"\u001b[39m, aug_p\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m augmented_texts \u001b[38;5;241m=\u001b[39m \u001b[43maug\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m print_and_highlight_diff(sample, augmented_texts)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/nlpaug/base_augmenter.py:119\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[0;34m(self, data, n, num_thread)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Single input with/without multiple input\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Single Thread\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_thread \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m         augmented_results \u001b[38;5;241m=\u001b[39m [action_fx(clean_data) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# Multi Thread\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         augmented_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parallel_augment(action_fx, clean_data, n\u001b[38;5;241m=\u001b[39mn, num_thread\u001b[38;5;241m=\u001b[39mnum_thread)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/nlpaug/base_augmenter.py:119\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Single input with/without multiple input\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Single Thread\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_thread \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m         augmented_results \u001b[38;5;241m=\u001b[39m [\u001b[43maction_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# Multi Thread\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         augmented_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parallel_augment(action_fx, clean_data, n\u001b[38;5;241m=\u001b[39mn, num_thread\u001b[38;5;241m=\u001b[39mnum_thread)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/nlpaug/augmenter/word/word_embs.py:147\u001b[0m, in \u001b[0;36mWordEmbsAug.substitute\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    144\u001b[0m change_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    145\u001b[0m doc \u001b[38;5;241m=\u001b[39m Doc(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(data))\n\u001b[0;32m--> 147\u001b[0m aug_idxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_aug_idxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_original_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aug_idxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(aug_idxes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_detail:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/nlpaug/augmenter/word/word_augmenter.py:77\u001b[0m, in \u001b[0;36mWordAugmenter._get_aug_idxes\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     75\u001b[0m aug_cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_aug_cnt(\u001b[38;5;28mlen\u001b[39m(tokens))\n\u001b[1;32m     76\u001b[0m word_idxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_skip_aug(tokens)\n\u001b[0;32m---> 77\u001b[0m word_idxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_aug\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_idxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word_idxes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/nlpaug/augmenter/word/word_embs.py:107\u001b[0m, in \u001b[0;36mWordEmbsAug.skip_aug\u001b[0;34m(self, token_idxes, tokens)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token_idx \u001b[38;5;129;01min\u001b[39;00m token_idxes:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Some words do not come with vector. It will be excluded in lucky draw.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     word \u001b[38;5;241m=\u001b[39m tokens[token_idx]\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab\u001b[49m():\n\u001b[1;32m    108\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(token_idx)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get_vocab'"
     ]
    }
   ],
   "source": [
    "# import nlpaug.augmenter.word as naw\n",
    "# aug = naw.WordEmbsAug(model_type='word2vec', model='nlpaug.model.word_embs.nmw.Word2vec()',action=\"substitute\", aug_p= 0.8)\n",
    "# augmented_texts = aug.augment(sample)\n",
    "# print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(naw.WordEmbsAug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.WordEmbsAug(\n",
    "    model_type='fasttext', model_path='./models/wiki-new-300d-1M.vec',\n",
    "    action=\"substitute\", aug_p= 0.8)\n",
    "aaugmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ContextualWordEmbs method\n",
    "\n",
    "I'll try SqueezeBERT since the resources are limited "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='distilbert-base-uncased', action=\"substitute\", aug_p= 0.3)\n",
    "augmented_text = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Original: 1\n",
      "Y\n",
      "Augmented: 5\n",
      "\u001b[31mmy\u001b[39m \u001b[31msample\u001b[39m \u001b[31mjust\u001b[39m \u001b[31mgoes\u001b[39m \u001b[31mhere\u001b[39m \n"
     ]
    }
   ],
   "source": [
    "aug = nawcwe.ContextualWordEmbsAug(\n",
    "    model_path=model_dir, \n",
    "    action ='substitute', \n",
    "    aug_p= 0.8)\n",
    "augmented_texts = aug.augment(sample)\n",
    "print_and_highlight_diff(sample, augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(nawcwe.ContextualWordEmbsAug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam-detector-P2ybB3t6-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
