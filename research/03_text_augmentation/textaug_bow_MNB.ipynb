{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to perform oversampling using different techniques to balance our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from mlflow.models import infer_signature\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', stream=sys.stdout)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train = pd.read_csv(\"../../data/gold/train.csv\")\n",
    "test = pd.read_csv(\"../../data/gold/test.csv\")\n",
    "val = pd.read_csv(\"../../data/gold/validation.csv\")\n",
    "\n",
    "X_train = train['features']\n",
    "y_train = train['target']\n",
    "X_test = test['features']\n",
    "y_test = test['target']\n",
    "\n",
    "\n",
    "signature = infer_signature(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    3606\n",
       "1     517\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    0.874606\n",
       "1    0.125394\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of oversampling with 1.25x: 0\n",
      "Size of oversampling with 1.5x: 0\n",
      "Size of oversampling with 2x: 0\n"
     ]
    }
   ],
   "source": [
    "oversample_factors = [1.25, 1.5, 2]\n",
    "oversample_sizes = [int(y_train[1] * factor) for factor in oversample_factors]\n",
    "\n",
    "for factor, size in zip(oversample_factors, oversample_sizes):\n",
    "    print(f\"Size of oversampling with {factor}x: {size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nlpAug techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../utils\"))\n",
    "\n",
    "from textaug_techniques import TextAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERSAMPLE_FACTORS = [1.25, 1.5, 2.0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running augmentation for factor 1.25x...\n",
      "2024-12-29 19:11:31,055 - INFO - Dataset '1.25x' created with 4252 samples.\n",
      "Running augmentation for factor 1.5x...\n",
      "2024-12-29 19:11:31,056 - INFO - Dataset '1.5x' created with 4381 samples.\n",
      "Running augmentation for factor 2.0x...\n",
      "2024-12-29 19:11:31,058 - INFO - Dataset '2.0x' created with 4640 samples.\n",
      "Dataset '1.25x':\n",
      "0      hey next sun number there basic yoga course bu...\n",
      "1                    dhoni luck win big titleso winemoji\n",
      "2                                     really hows master\n",
      "3                               see cup coffee animation\n",
      "4                           pain couldnt come worse time\n",
      "                             ...                        \n",
      "124    qw_augmented_dear voucher holder number claim ...\n",
      "125               qw_augmented_filthy story girl waiting\n",
      "126    qw_augmented_freemsg today day ready im horny ...\n",
      "127    qw_augmented_please call number immediately ur...\n",
      "128    qw_augmented_urgent please call number landlin...\n",
      "Name: features, Length: 4252, dtype: object\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "124    1\n",
      "125    1\n",
      "126    1\n",
      "127    1\n",
      "128    1\n",
      "Length: 4252, dtype: int64\n",
      "Dataset '1.5x':\n",
      "0      hey next sun number there basic yoga course bu...\n",
      "1                    dhoni luck win big titleso winemoji\n",
      "2                                     really hows master\n",
      "3                               see cup coffee animation\n",
      "4                           pain couldnt come worse time\n",
      "                             ...                        \n",
      "253         qw_augmented_freemsgfav xmas tonesreply real\n",
      "254    qw_augmented_urgent please call number landlin...\n",
      "255    qw_augmented_thanks ringtone order reference n...\n",
      "256    qw_augmented_freeringtone reply real poly eg r...\n",
      "257    qw_augmented_romcapspam everyone around respon...\n",
      "Name: features, Length: 4381, dtype: object\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "253    1\n",
      "254    1\n",
      "255    1\n",
      "256    1\n",
      "257    1\n",
      "Length: 4381, dtype: int64\n",
      "Dataset '2.0x':\n",
      "0      hey next sun number there basic yoga course bu...\n",
      "1                    dhoni luck win big titleso winemoji\n",
      "2                                     really hows master\n",
      "3                               see cup coffee animation\n",
      "4                           pain couldnt come worse time\n",
      "                             ...                        \n",
      "512      qw_augmented_unique enough find numberth august\n",
      "513    qw_augmented_number nokia tone number ur mob e...\n",
      "514    qw_augmented_textnumber get ringtones logo gam...\n",
      "515    qw_augmented_ever thought living good life per...\n",
      "516    qw_augmented_want number get laid tonight want...\n",
      "Name: features, Length: 4640, dtype: object\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "512    1\n",
      "513    1\n",
      "514    1\n",
      "515    1\n",
      "516    1\n",
      "Length: 4640, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "augmenter = TextAugmentation(qw=1, aa=0, cwea=0, sa=0, bta=0, wea=0)\n",
    "all_datasets = {}\n",
    "\n",
    "\n",
    "for factor in OVERSAMPLE_FACTORS:\n",
    "    print(f\"Running augmentation for factor {factor}x...\")\n",
    "    datasets = augmenter.augment(X_train, y_train, factor)\n",
    "    if datasets:\n",
    "        all_datasets.update(datasets)  # Combinar resultados\n",
    "\n",
    "# Mostrar los resultados\n",
    "for label, (aug_X, aug_y) in all_datasets.items():\n",
    "    print(f\"Dataset '{label}':\")\n",
    "    print(aug_X)\n",
    "    print(aug_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLFlow config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on the console\n",
    "# mlflow server --backend-store-uri sqlite:///backend.db --default-artifact-root ./artifacts\n",
    "# mlflow.search_experiments()\n",
    "\n",
    "# from mlflow.tracking import MlflowClient\n",
    "# client = MlflowClient()\n",
    "# client.delete_run(\"81b0d40ddc814076a95bc6fd9d4fff34\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    fbeta_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    balanced_accuracy_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(ngram_range=(1, 1), max_features=None)),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    \n",
    "pipeline.fit(X_train, y_train)\n",
    "y_train_pred = pipeline.predict(X_train)\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Train Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham      0.995     0.995     0.995      3606\n",
      "        spam      0.967     0.967     0.967       517\n",
      "\n",
      "    accuracy                          0.992      4123\n",
      "   macro avg      0.981     0.981     0.981      4123\n",
      "weighted avg      0.992     0.992     0.992      4123\n",
      "\n",
      "Classification Report (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham      0.993     0.982     0.988       453\n",
      "        spam      0.882     0.952     0.916        63\n",
      "\n",
      "    accuracy                          0.979       516\n",
      "   macro avg      0.938     0.967     0.952       516\n",
      "weighted avg      0.980     0.979     0.979       516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_report = classification_report(y_train, y_train_pred, target_names = ['ham', 'spam'], digits=3)\n",
    "print(\"Classification Report (Train Data):\")\n",
    "print(train_report)\n",
    "\n",
    "test_report = classification_report(y_test, y_test_pred, target_names = ['ham', 'spam'], digits=3)\n",
    "print(\"Classification Report (Test Data):\")\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def aug_KeyboardAug(X_train, y_train, oversample_factor):\n",
    "#     augmenter = nac.KeyboardAug()  \n",
    "    \n",
    "#     positive_samples = X_train[y_train == 1]\n",
    "\n",
    "#     augmented_texts = []\n",
    "\n",
    "#     # Calcular cuántos ejemplos de spam quieres generar\n",
    "#     num_augmentations = int(len(positive_samples) * oversample_factor) - len(positive_samples)\n",
    "\n",
    "#     # Generar los ejemplos aumentados\n",
    "#     while len(augmented_texts) < num_augmentations:\n",
    "#         for message in positive_samples:\n",
    "#             augmented_text = augmenter.augment(message)\n",
    "#             augmented_texts.append(augmented_text)\n",
    "#             if len(augmented_texts) >= num_augmentations:\n",
    "#                 break\n",
    "\n",
    "#     augmented_series = pd.Series(augmented_texts, name=X_train.name)\n",
    "\n",
    "#     # Crear una Series con las etiquetas correspondientes\n",
    "#     augmented_labels = pd.Series([1] * len(augmented_series), index=augmented_series.index)\n",
    "\n",
    "#     # Combinar los datos augmentados con el conjunto original\n",
    "#     X_train_aug = pd.concat([X_train, augmented_series])\n",
    "#     y_train_aug = pd.concat([y_train, augmented_labels])\n",
    "\n",
    "#     return X_train_aug, y_train_aug\n",
    "\n",
    "\n",
    "# # Lista de factores de oversampling que deseas probar\n",
    "# oversample_factors = [1.25, 1.5, 1.75, 2]\n",
    "\n",
    "# # Suponiendo que X_train y y_train ya están definidos en tu código\n",
    "\n",
    "# # Crear un diccionario para almacenar los resultados de oversampling para cada factor\n",
    "# oversampled_data = {}\n",
    "\n",
    "# # Ejecutar la función aug_KeyboardAug para cada factor\n",
    "# for factor in oversample_factors:\n",
    "#     print(f\"Ejecutando oversampling con factor {factor}x...\")\n",
    "#     X_train_aug, y_train_aug = aug_KeyboardAug(X_train, y_train, factor)\n",
    "    \n",
    "#     # Almacenar los datos aumentados en el diccionario\n",
    "#     oversampled_data[factor] = (X_train_aug, y_train_aug)\n",
    "    \n",
    "#     # Imprimir el tamaño de las clases después del oversampling\n",
    "#     print(f\"Tamaño de clase ham después del oversampling: {sum(y_train_aug == 0)}\")\n",
    "#     print(f\"Tamaño de clase spam después del oversampling: {sum(y_train_aug == 1)}\")\n",
    "#     print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'naw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m aug \u001b[38;5;241m=\u001b[39m \u001b[43mnaw\u001b[49m\u001b[38;5;241m.\u001b[39mSynonymAug(aug_src\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m,aug_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'naw' is not defined"
     ]
    }
   ],
   "source": [
    "aug = naw.SynonymAug(aug_src='wordnet',aug_max=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam-detector-P2ybB3t6-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
