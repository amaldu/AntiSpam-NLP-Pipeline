{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to create files with the vectorized representation of the words for three different types: BOW, TF-IDF, word2vec to save time and resources (my pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>20</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entry number wkly comp win fa cup final t...</td>\n",
       "      <td>28</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>11</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah dont think go usf life around though</td>\n",
       "      <td>13</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5149</th>\n",
       "      <td>1</td>\n",
       "      <td>numbernd time tried number contact u u poundnu...</td>\n",
       "      <td>30</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5150</th>\n",
       "      <td>0</td>\n",
       "      <td>b going esplanade fr home</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5151</th>\n",
       "      <td>0</td>\n",
       "      <td>pity mood soany suggestion</td>\n",
       "      <td>10</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5152</th>\n",
       "      <td>0</td>\n",
       "      <td>guy bitching acted like id interested buying s...</td>\n",
       "      <td>26</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5153</th>\n",
       "      <td>0</td>\n",
       "      <td>rofl true name</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5154 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Category                                            Message  word_count  \\\n",
       "0            0  go jurong point crazy available bugis n great ...          20   \n",
       "1            0                            ok lar joking wif u oni           6   \n",
       "2            1  free entry number wkly comp win fa cup final t...          28   \n",
       "3            0                u dun say early hor u c already say          11   \n",
       "4            0           nah dont think go usf life around though          13   \n",
       "...        ...                                                ...         ...   \n",
       "5149         1  numbernd time tried number contact u u poundnu...          30   \n",
       "5150         0                          b going esplanade fr home           8   \n",
       "5151         0                         pity mood soany suggestion          10   \n",
       "5152         0  guy bitching acted like id interested buying s...          26   \n",
       "5153         0                                     rofl true name           6   \n",
       "\n",
       "      char_count  \n",
       "0            111  \n",
       "1             29  \n",
       "2            155  \n",
       "3             49  \n",
       "4             61  \n",
       "...          ...  \n",
       "5149         160  \n",
       "5150          36  \n",
       "5151          57  \n",
       "5152         125  \n",
       "5153          26  \n",
       "\n",
       "[5154 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/silver/df_preprocessed.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Message']\n",
    "y = df['Category']  \n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_bow(X_train, X_val, X_test):\n",
    "    vectorizer = CountVectorizer(ngram_range= (1,3), max_features = 2000)\n",
    "    X_train_bow = vectorizer.fit_transform(X_train)\n",
    "    X_val_bow = vectorizer.transform(X_val)\n",
    "    X_test_bow = vectorizer.transform(X_test)\n",
    "    \n",
    "    return vectorizer, X_train_bow, X_val_bow, X_test_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tfidf(X_train, X_val, X_test):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=2000)\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_val_tfidf = vectorizer.transform(X_val)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    return vectorizer, X_train_tfidf, X_val_tfidf, X_test_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer and TfidfVectorizer tokenize the text already but word2vec no so we do it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/maldu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general dimension of the vector = 100 is fine for a small dataset\n",
    "# max number of words considered as context of each word. Let's try with 5 by default.      \n",
    "# min frequency of word appearance in the corpus. All words are included for now.             \n",
    "# threads to accelerate the training. I have an Intel core i7 so I gonna use 6 to work in parallel \n",
    "\n",
    "def preprocess_word2vec(X_train, X_val, X_test):\n",
    "    \n",
    "    corpus_train = [word_tokenize(text.lower()) for text in X_train]\n",
    "    corpus_val = [word_tokenize(text.lower()) for text in X_val]\n",
    "    corpus_test = [word_tokenize(text.lower()) for text in X_test]\n",
    "    \n",
    "    model = Word2Vec(sentences=corpus_train, vector_size=100, window=5, min_count=1, workers=os.cpu_count() - 1)\n",
    "    \n",
    "    # Función para obtener el promedio de los vectores de palabras\n",
    "    def get_average_word2vec(tokens_list, model, vector_size=100):\n",
    "        valid_words = [word for word in tokens_list if word in model.wv]\n",
    "        if valid_words:\n",
    "            word_vectors = [model.wv[word] for word in valid_words]\n",
    "            return np.mean(word_vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(vector_size)\n",
    "    \n",
    "    X_train_word2vec = np.array([get_average_word2vec(tokens, model) for tokens in corpus_train])\n",
    "    X_val_word2vec = np.array([get_average_word2vec(tokens, model) for tokens in corpus_val])\n",
    "    X_test_word2vec = np.array([get_average_word2vec(tokens, model) for tokens in corpus_test])\n",
    "    \n",
    "    return model, X_train_word2vec, X_val_word2vec, X_test_word2vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam-detector-P2ybB3t6-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
